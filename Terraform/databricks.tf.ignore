/*resource "databricks_notebook" "this" {
  path     = "${data.databricks_current_user.me.home}/${var.notebook_subdirectory}/${var.notebook_filename}"
  language = var.notebook_language
  source   = "./${var.notebook_filename}"
}

resource "databricks_cluster" "this" {
  cluster_name            = var.cluster_name
  node_type_id            = data.databricks_node_type.smallest.id
  spark_version           = data.databricks_spark_version.latest_lts.id
  autotermination_minutes = var.cluster_autotermination_minutes
  num_workers             = var.cluster_num_workers
}
*/
resource "databricks_job" "this" {
  depends_on = [azurerm_databricks_workspace.workspace]
  name = var.job_name
  timeout_seconds = 3060
  max_retries = 1
  max_concurrent_runs = 1
  job_cluster {
    job_cluster_key = "j"
    new_cluster {
      num_workers = 1
      spark_version = "7.3.x-scala2.12"
      node_type_id = "Standard_DS3_v2"
    }
  }
  #existing_cluster_id = databricks_cluster.this.cluster_id
  #notebook_task {
  #  notebook_path = databricks_notebook.this.path
  #}
  task {
    task_key = "RetrieveBabyNames"
    new_cluster {
      num_workers = 1
      spark_version = "7.3.x-scala2.12"
      node_type_id = "Standard_DS3_v2"
    }
    notebook_task {
      notebook_path = var.notebook_path_RetreiveBabyNames  #databricks_notebook.this.path
    }
  }
  email_notifications {
    #on_success = [ data.databricks_current_user.me.user_name ]
    #on_failure = [ data.databricks_current_user.me.user_name ]
    no_alert_for_skipped_runs = true
  }
}

resource "azurerm_databricks_workspace" "workspace"{
  location = azurerm_resource_group.resourcegroup.location
  name = var.databricks_name
  resource_group_name = azurerm_resource_group.resourcegroup.name
  sku = "premium"

}
resource "databricks_user" "mohamed"{
  depends_on = [azurerm_databricks_workspace.workspace]
  user_name = "mohamed.darwish@intergen.co.nz"
  display_name = "Admin user"
}
#data "databricks_node_type" "smallest"{
#  depends_on = [azurerm_databricks_workspace.workspace]
#  local_disk = true
#}
#data "databricks_spark_version" "latest_lts"{
#  depends_on = [azurerm_databricks_workspace.workspace]
#  long_term_support = true
#}

#resource "databricks_instance_pool" "pool"{
#  instance_pool_name = "CodeRedPool"
#  min_idle_instances = 0
#  max_capacity = 10
#  node_type_id = data.databricks_node_type.smallest.id
#  idle_instance_autotermination_minutes = 10
#}

resource "databricks_cluster" "shared_autoscalling"{
  depends_on = [azurerm_databricks_workspace.workspace]
  #instance_pool_id = databricks_instance_pool.pool.id
  cluster_name = "Shared Autoscaling"
  spark_version = "7.3.x-scala2.12" #data.databricks_spark_version.latest_lts.id
  node_type_id = "Standard_DS3_v2" #data.databricks_node_type.smallest.id
  autotermination_minutes = 20
  autoscale{
    min_workers = 1
    max_workers = 50
  }
  #spark_conf = {
  #  "spark.databricks.io.cache.enabled" : true
  #}
  custom_tags = var.tags
  #tags = var.tags
}

resource "databricks_notebook" "this" {
  depends_on = [azurerm_databricks_workspace.workspace]
  #path     = "${data.databricks_current_user.me.home}/${var.notebook_subdirectory}/${var.notebook_filename}"
  language = var.notebook_language
  path = var.notebook_path_RetreiveBabyNames
  content_base64 = base64encode("print('Hello Databricks')")
  #source   = "./${var.notebook_filename}"
}
/*
resource "databricks_cluster" "this" {
  cluster_name            = var.cluster_name
  node_type_id            = data.databricks_node_type.smallest.id
  spark_version           = data.databricks_spark_version.latest_lts.id
  autotermination_minutes = var.cluster_autotermination_minutes
  num_workers             = var.cluster_num_workers
}*/